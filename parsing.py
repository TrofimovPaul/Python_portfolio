# -*- coding: utf-8 -*-
"""Parsing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EoX1MXTlHaLKlSURgswt02s03dAS90Vl
"""

"""Обязательная часть

Вам необходимо написать функцию, которая будет основана на поиске по сайту habr.com. 
Функция в качестве параметра должна принимать список запросов для поиска (например, ['python', ''])
и на основе материалов, попавших в результаты поиска по каждому запросу, возвращать датафрейм вида:

<дата> - <заголовок> - <ссылка на материал>

В рамках задания предполагается работа только с одной (первой) страницей результатов поисковой выдачи для каждого запроса.
Материалы в датафрейме не должны дублироваться, если они попадали в результаты поиска для нескольких запросов из списка."""


"""Дополнительная часть (необязательная)

Функция из обязательной части задания должна быть расширена следующим образом:
кроме списка ключевых слов для поиска необходимо объявить параметр с количеством страниц поисковой выдачи. 
Т.е. при передаче в функцию аргумента 4 необходимо получить материалы с первых 4 страниц результатов;
в датафрейме должны быть столбцы с полным текстом найденных материалов и количеством лайков:

    <дата> - <заголовок> - <ссылка на материал> - <текст материала> - <количество лайков>"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
def habr_parsing():
  """
  answer - [1 - ввод запроса, 0 - отказ от ввода]
  n - количество страниц для поиска

  Проводит поиск на https://habr.com/ru/search/ по указанным запросам с учётом количества страниц. 
  Выводит  датафрейм в виде:
  <дата> - <заголовок> - <ссылка на материал> - <текст материала> - <количество лайков>
  """
  
  #Первоначальный вопрос
  answer = int(input('Хотите ввести запрос? 1 - Да, 0 - Нет\n'))
  
  #Если отказались от ввода
  if answer == 0:
    print('Вы ничего не ввели :(')
  search_list = []
  
  #Ввод запросов для поиска
  while answer == 1:
    new_query = input('Введите запрос для поиска: ')
    search_list.append(new_query)
    #print(f'Теперь Ваш список выглядит так:\n{search_list}')
    answer = int(input('Хотите ввести запрос? 1 - Да, 0 - Нет\n'))
  
  #Вывод конечного списка запросов для поиска
  print(f'Ваш конечный список для поиска выглядит так:\n{search_list}')
  
  #Ввод количества страниц для поиска
  n = int(input('Введите количество страниц: '))
  number_of_pages = range(1,n+1)
  
  URL_template = 'https://habr.com/ru/search/page' 
  headers = {
      'User-Agent':	'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0'
  }
  search_dict = {}
  habr_df = pd.DataFrame()
  #Проход по страницам
  for num in number_of_pages:
    URL = 'https://habr.com/ru/search/page' + str(num)
    #Запись в словарь (ключ - поисковый запрос, значение - код запроса) 
    for query in search_list:
      params = {
        'q': query
      }
      search_page = requests.get(URL, params = params, headers = headers)
      time.sleep(0.33)
      search_dict[query] = search_page
    #print(search_dict)
    #Проход по значениям словаря
    for k, v in search_dict.items():
      v_new = v.text
      soup = BeautifulSoup(v_new)
      articles = soup.find_all('article', class_='tm-articles-list__item')
      print(f'Найдено публикаций для "{k}": {len(articles)}')
      title = f'Публикации для "{k}". Количество: {len(articles)}. Страница №{num}'
      link = '- - -'
      date = '- - -'
      main_info = '- - -'
      likes = '- - -'
      row = {'date': date, 'title': title, 'link': link, 'info': main_info, 'likes': likes}
      habr_df = pd.concat([habr_df, pd.DataFrame([row])])
      #Проход по статьям на странице
      for article in articles:
        try:
          title = article.find('a', class_='tm-article-snippet__title-link').find('span').text
        except:
          title = article.find('h2', class_='tm-megapost-snippet__title').text
        #print(title)
        try:
          link = 'https://habr.com' + article.find('a', class_='tm-article-snippet__title-link').get('href')
        except:
          link = 'https://habr.com' + article.find('a', class_='tm-megapost-snippet__link tm-megapost-snippet__card').get('href')
        try:
          date = article.find('span', class_='tm-article-snippet__datetime-published').text.strip()
        except:
          date = article.find('a', class_='tm-megapost-snippet__link tm-megapost-snippet__date').text.strip()
        open_page = requests.get(link).text
        time.sleep(0.33)
        soup4open = BeautifulSoup(open_page)
        main_info = soup4open.find('div', class_='tm-article-body').text
        try:
          likes = soup4open.find('span', class_='tm-votes-meter__value tm-votes-meter__value tm-votes-meter__value_positive tm-votes-meter__value_appearance-article tm-votes-meter__value_rating').text
        except:
          continue
        row = {'date': date, 'title': title, 'link': link, 'info': main_info, 'likes': likes}
        if link not in list(habr_df['link']):
          habr_df = pd.concat([habr_df, pd.DataFrame([row])])
        else:
          continue
    habr_df = habr_df.reset_index()
    habr_df = habr_df.drop(columns = ['index'], axis = 1)
  return habr_df
habr_parsing()

